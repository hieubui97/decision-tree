# **N·ªôi dung b√†i th·ª±c h√†nh**
#
# Ng∆∞·ªùi h·ªçc ti·∫øp c·∫≠n v√† gi·∫£i quy·∫øt b√†i to√°n ph√¢n l·ªõp vƒÉn b·∫£n s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p Naive Bayser. Sau khi th·ª±c h√†nh, ng∆∞·ªùi h·ªçc c√≥ kh·∫£ nƒÉng:
# 1. S·ª≠ d·ª•ng ƒë∆∞·ª£c c√°c c√¥ng c·ª• c·ªßa sklearn cho b√†i to√°n ph√¢n l·ªõp
#
# *   Thao t√°c v·ªõi d·ªØ li·ªáu
# *   Chuy·ªÉn t·ª´ vƒÉn b·∫£n sang kh√¥ng gian vector
#
# 2. √Åp d·ª•ng ƒë∆∞·ª£c NB cho b√†i to√°n ph√¢n l·ªõp
#
# *   Hu·∫•n luy·ªán m√¥ h√¨nh
# *   ƒê√°nh g√≠a m√¥ h√¨nh
#
# 3. C·∫£i ti·∫øn ƒë∆∞·ª£c m√¥ h√¨nh ph√¢n l·ªõp
# 3. Th·ª±c h√†nh ƒë∆∞·ª£c v·ªõi b√†i to√°n th·ª±c t·∫ø

# **Thao t√°c v·ªõi d·ªØ li·ªáu**

# *   B√†i th·ª±c h√†nh s·ª≠ d·ª•ng d·ªØ li·ªáu ti·∫øng Anh
# *   D·ªØ li·ªáu g·ªìm 18.000 b√†i b√°o ƒë∆∞·ª£c t·ªï ch·ª©c trong 20 l·ªõp (classes/groups)
# *   Y√™u c·∫ßu: x√¢y d·ª±ng m√¥ h√¨nh ph√¢n l·ªõp c√°c b√†i b√°o d·ª±a tr√™n m√¥ h√¨nh h·ªçc m√°y Naive Bayes




# **ƒê·ªçc d·ªØ li·ªáu**

# In[1]:
import warnings
warnings.filterwarnings('ignore')

# Loading the dateset - training data
from sklearn.datasets import fetch_20newsgroups
twenty_train = fetch_20newsgroups(subset='train', shuffle=True)

# **Quan s√°t d·ªØ li·ªáu hu·∫•n luy·ªán**

# *   S·ªë l∆∞·ª£ng d·ªØ li·ªáu hu·∫•n luy·ªán
# *   5 vƒÉn b·∫£n ƒë·∫ßu ti√™n

print("The number of training example:", len(twenty_train.data))

# printing top five training examples
print(twenty_train.data[0:5])

# Danh s√°ch c√°c l·ªõp target_name
# You can check the target names (categories) and some data files by following commands.
# print all the categories
print('\nDanh s√°ch c√°c l·ªõp:\n{}\n'.format(twenty_train.target_names))

# Nh√£n c·ªßa c√°c l·ªõp target
targets = twenty_train.target
print('Nh√£n c·ªßa c√°c l·ªõp:\n{}\n'.format(targets))
print('len(targets): ', len(targets))

# Hi·ªÉn th·ªã d√≤ng ƒë·∫ßu ti√™n c·ªßa vƒÉn b·∫£n ƒë·∫ßu ti√™n
print("\n\n".join(twenty_train.data[0].split("\n")[:3]))


# **Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán**
#
# Ph·∫ßn chu·∫©n b·ªã d·ªØ li·ªáu cho hu·∫•n luy·ªán m√¥ h√¨nh l√† vi·ªác chuy·ªÉn c√°c vƒÉn b·∫£n v√†o kh√¥ng gian ƒë·∫∑c tr∆∞ng (vector space model)
#
# *   ƒê·∫∑c tr∆∞ng: s·ª≠ d·ª•ng term frequency (TF) ho·∫∑c TF-IDF
# *   ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa c√°c t·ª´
# *   D·ª±a tr√™n bag-of-words model
#
# *   T·∫°o ma tr·∫≠n term-document, trong ƒë√≥ gi√° tr·ªã ·ªü m·ªói √¥ l√† s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ trong vƒÉn b·∫£n ch·ª©a n√≥

# Extracting features from text files CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(twenty_train.data)

print(X_train_counts.shape)
print(X_train_counts[0])

# Bi·ªÉu di·ªÖn vƒÉn b·∫£n b·∫±ng TF-IDF
from  sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

print(X_train_tfidf.shape)
print(X_train_tfidf[0])

# **Hu·∫•n luy·ªán m√¥ h√¨nh**
#
# *   Kh·ªüi t·∫°o ƒë·ªëi t∆∞·ª£ng
# *   Hu·∫•n luy·ªán m√¥ h√¨nh
# *   Gi·∫£i c√°c tham s·ªë c·ªßa NB

# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:
# The names ‚Äòvect‚Äô , ‚Äòtfidf‚Äô and ‚Äòclf‚Äô are arbitrary but will be used later.
# We will be using the 'text_clf' going forward.

from  sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB

text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', MultinomialNB())])
text_clf = text_clf.fit(twenty_train.data, twenty_train.target)


# **L∆∞u √Ω khi thi·∫øt l·∫≠p th·ª±c nghi·ªám**
#
# ·ªû ƒë√¢y, ch√∫ng ta s·ª≠ d·ª•ng c√°ch chia d·ªØ li·ªáu l√†m 2 ph·∫ßn, training-test.
# Ch√∫ng ta c≈©ng c√≥ th·ªÉ s·ª≠ d·ª•ng c√°ch k-fold cross-valiation

# **Tr·ª±c quan h√≥a qu√° tr√¨nh hu·∫•n luy·ªán c·ªßa NB**

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import ShuffleSplit

def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):
    """
        Generate a simple plot of the test and training learning curve.

        Parameters
        ----------
        estimator : object type that implements the "fit" and "predict" methods
            An object of that type which is cloned for each validation.

        title : string
            Title for the chart.

        X : array-like, shape (n_samples, n_features)
            Training vector, where n_samples is the number of samples and
            n_features is the number of features.

        y : array-like, shape (n_samples) or (n_samples, n_features), optional
            Target relative to X for classification or regression;
            None for unsupervised learning.

        ylim : tuple, shape (ymin, ymax), optional
            Defines minimum and maximum yvalues plotted.

        cv : int, cross-validation generator or an iterable, optional
            Determines the cross-validation splitting strategy.
            Possible inputs for cv are:
              - None, to use the default 3-fold cross-validation,
              - integer, to specify the number of folds.
              - An object to be used as a cross-validation generator.
              - An iterable yielding train/test splits.

            For integer/None inputs, if ``y`` is binary or multiclass,
            :class:`StratifiedKFold` used. If the estimator is not a classifier
            or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.

            Refer :ref:`User Guide <cross_validation>` for the various
            cross-validators that can be used here.

        n_jobs : integer, optional
            Number of jobs to run in parallel (default 1).
        """
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color="r")

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    return plt


# In[12]:

estimator = MultinomialNB()
title = "Learning Curves (Naive Bayes)"

# Cross validation with 100 iterations to get smoother mean test and train
# score curves, each time with 20% data randomly selected as a validation set.
cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
X, y = X_train_tfidf, twenty_train.target
plot_learning_curve(estimator, title, X, y, ylim=(0.0, 1.01), cv=cv, n_jobs=8)
plt.show()


# **ƒê√°nh gi√° m√¥ h√¨nh tr√™n d·ªØ li·ªáu test**

# In[13]:

import numpy as np
twenty_test = fetch_20newsgroups(subset='test', shuffle=True)
predicted  = text_clf.predict(twenty_test.data)
np.mean(predicted == twenty_test.target)

print(np.mean(predicted == twenty_test.target))

# Confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(twenty_test.target, predicted)
plt.figure(figsize=(10, 10))
plt.imshow(cm, cmap="Reds")
plt.show()
print(cm)

# **C·∫£i ti·∫øn m√¥ h√¨nh**
#
# Hi·ªáu qu·∫£ c·ªßa m√¥ h√¨nh c√≥ th·ªÉ c·∫£i ti·∫øn b·∫±ng nhi·ªÅu ph∆∞∆°ng ph√°p, trong ƒë√≥ m·ªôt trong nh·ªØng ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n l√† c·∫£i ti·∫øn qu√° tr√¨nh chuy·ªÉn t·ª´ vƒÉn b·∫£n sang kh√¥ng gian vector. Trong ph·∫ßn n√†y, m√¥ h√¨nh s·∫Ω ƒë∆∞·ª£c c·∫£i ti·∫øng b·∫±ng c√°ch s·ª≠ d·ª•ng:
#
# *   Lo·∫°i b·ªè c√°c t·ª´ d·ª´ng
# *   ƒê∆∞a m·ªôt t·ª´ v·ªÅ t·ª´ g·ªëc
#
# *   Kh·ªüi t·∫°o m√¥ h√¨nh c√≥ d√πng th√™m tham s·ªë lo·∫°i b·ªè ƒëi c√°c t·ª´ d·ª´ng

# In[17]:
# NLTK
# Removing stop words
from sklearn.pipeline import Pipeline
text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()),
                     ('clf', MultinomialNB())])


# *   S·ª≠ d·ª•ng stemming, v√† hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh
# *   When downloading NLTK, you mal use *all* to download all packages

# In[20]:
# Stemming Code
import nltk
nltk.download('stopwords')

# In[21]:
print('steming the corpus... Please wait...')

from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english", ignore_stopwords=True)

class StemmedCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer, self).build_analyzer()
        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])

stemmed_count_vect = StemmedCountVectorizer(stop_words='english')

text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()),
                             ('mnb', MultinomialNB(fit_prior=False))])

text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)

predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)

np.mean(predicted_mnb_stemmed == twenty_test.target)

print(np.mean(predicted_mnb_stemmed == twenty_test.target))



# **So s√°nh v·ªõi ph∆∞∆°ng ph√°p ph√¢n lo·∫°i SVM**

# In[22]:
# Training Support Vector Machines - SVM and calculating its performance
from sklearn.linear_model import SGDClassifier
text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),
                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter_no_change=5,
                                                   random_state=42))])

text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)

predicted_svm = text_clf_svm.predict(twenty_test.data)

np.mean(predicted_svm == twenty_test.target)

print(np.mean(predicted_svm == twenty_test.target))


# **Tr·ª±c quan ho√° qu√° tr√¨nh hu·∫•n luy·ªán c·ªßa NB v√† SVM**

# In[23]:

estimator = MultinomialNB()
title = "Learning Curves (Naive Bayes)"

# Cross validation with 100 iterations to get smoother mean test and train
# score curves, each time with 20% data randomly selected as a validation set.

cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
X, y = X_train_tfidf, twenty_train.target
plot_learning_curve(estimator, title, X, y, ylim=(0.0, 0.95), cv=cv, n_jobs=8)

#from sklearn.svm import SVC

from sklearn.linear_model import SGDClassifier

#title = "Learning Curves (SVM, RBF kernel, $\gamma=0.001$)"
title = "Learning Curves (SVM, linear kernel)"

# SVC is more expensive so we do a lower number of CV iterations:
cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)

#estimator = SVC(gamma=0.001)

estimator = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter_no_change=5, random_state=42, verbose=0)

plot_learning_curve(estimator, title, X, y, ylim=(0.5, 1.1), cv=cv, n_jobs=8)

plt.show()


# **S·ª≠ d·ª•ng GridSearch ƒë·ªÉ t√¨m tham s·ªë ph√π h·ª£p**
#
# C√≥ th·ªÉ s·ª≠ d·ª•ng thu·∫≠t to√°n GridSearch ƒë·ªÉ t√¨m tham s·ªë ph√π h·ª£p--> tƒÉng ƒë·ªô t·ªët c·ªßa m√¥ h√¨nh.
# Tuy nhi√™n, thu·∫≠t to√°n n√†y c√≥ nh∆∞·ª£c ƒëi·ªÉm l√† t·ªëc ƒë·ªô ch·∫≠m n√™n c√≥ th·ªÉ ph√π h·ª£p v·ªõi b·ªô d·ªØ li·ªáu nh·ªè.
# V·ªõi b·ªô d·ªØ li·ªáu l·ªõn thu·∫≠t to√°n ch·∫°y trong th·ªùi gian l√¢u

# In[24]:

# Grid Search
# Here, we are creating a list of parameters for which we would like to do performance tuning.
# All the parameters name start with the classifier name (remember the arbitrary name we gave).
# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.

from sklearn.model_selection import GridSearchCV
parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}


# In[25]:

# Next, we create an instance of the grid search by passing the classifier, parameters
# and n_jobs=-1 which tells to use multiple cores from user machine.

gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)
gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)


# In[26]:

# To see the best mean score and the params, run the following code

gs_clf.best_score_
gs_clf.best_params_

print(gs_clf.best_score_)
print(gs_clf.best_params_)

# Output for above should be: The accuracy has now increased to ~90.6% for the NB classifier (not so naive anymore! üòÑ)
# and the corresponding parameters are {‚Äòclf__alpha‚Äô: 0.01, ‚Äòtfidf__use_idf‚Äô: True, ‚Äòvect__ngram_range‚Äô: (1, 2)}.


# In[27]:

# Similarly doing grid search for SVM
from sklearn.model_selection import GridSearchCV
parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-2, 1e-3)}

gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)
gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)


gs_clf_svm.best_score_
gs_clf_svm.best_params_

print(gs_clf_svm.best_score_)
print(gs_clf_svm.best_params_)